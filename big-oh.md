# Notes from Harvard CS50 course

What makes a program fast or slow? The speed of a computer program is `not measured in seconds or minutes` because computer hardware and software is drastically different.

The speed is measured using Asymptotic Complexity of a program and Big-O Notation.

Formally, a function $f(x)$ runs in the order of $g(x)$ if there exists a value $x_0$ and a constant $C$ such that $f(x) \leq C\cdot g(x) \enspace \forall \enspace x > x_0$.

$$
f(x) \leq C\cdot g(x) \enspace \forall \enspace x > x_0
$$





